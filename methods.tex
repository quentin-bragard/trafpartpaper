\section{SPACE PARTITIONING}
\label{sec:spacePart}

\subsection{TwoTree partitioning}

TwoTree partitioning algorithm is a really simple and naive space partitioning algorithm. The algorithm consists in continuously dividing the space and subspaces in two until each regions are under an arbitrary threshold or it reaches the number of regions requested. After a division, the algorithm will look for the biggest region existing and divide it in two. The division will be done horizontally and vertically alternatively. Once the division scheme is complete, the algorithm will determine in which region belong the vertices using their GPS coordinates. The algorithm forms quickly rectangular-shaped regions with few neighbours and a small perimeters.

\subsection{Smart QuadTree partitioning}

Smart QuadTree (SQT) works with two different phases: \textit{Division} and \textit{Gathering}. The method \textit{Division} consists in dividing the space into the smallest tiles possible by partitioning each subspace in four equal subspaces according to their GPS coordinates. The second method, \textit{Gathering} will, then, use a region growing technique to recreate the region. First, one seed per regions requested, is chosen among the tiles according the predefined heuristics (in our case, hierarchical level of the roads present on the tile). Then one after another, regions will aggregate neighbours tiles until they cannot grow any longer due to the boundaries of the graph or the boundaries of other regions. The algorithm forms complex-shaped regions optimising the load-balancing of the system.

\section{TRAFFIC ROAD PARTITIONING: SPARTSIM}
\label{sec:spartsim}

SParTSim works as a multi-level region growing road network partitioning.
We start by choosing specific intersections which will be act as seeds, then let the regions growing by aggregating vertices around the region. All regions are growing concurrently until they reach the boundaries of neighbouring regions. 

SParTSim works in three step: \textit{Growing}, \textit{Trading} and \textit{Connecting}. Algorithm \ref{algo:spartsim} presents in lines 6-11 the growing method. The function \verb+BestCandidateVertex+, presented line 4, uses predefined heuristics (natural bridges, highest road hierarchy, degree of the vertex) to choose the best seed for a region. Our current work uses only the degree of the vertex assuming that more cars go through vertices with highest degree. However, we are currently doing works on data analysis to identify more accurately what could be the best seed. Then, function \verb+Grow+ (line 9) takes care of the growing part of SParTSim. The method gathers the vertices on the outside border of the partition $\gamma_i$ and rank them according to the hierarchical level of the road segment, the length of the segment, and whether this neighbour already belongs to this partition, to another partition or has not been selected yet. By default, the algorithm always picks the segment with the highest hierarchical level and uses the other criteria to decide between two equivalent segments. However the criteria can be weighted to optimise the partitioning according to the scenario. The method \verb+Grow+ stops when no region is able to grow any longer. After \verb+Grow+, the method \verb+Trade+ allows the regions to exchange vertices in order to load-balance the partitioning as much as possible. Exchange are made between overloaded regions and under-loaded regions and conducted through the shortest path between the two regions. This method will run until the partitioning reaches an acceptable level of unbalance. On the downside, \verb+Trade+ may introduce unconnected sub-partition belonging to the same region. The last part of the algorithm tries to recreate the connectivity at the price of some load-balance. First \verb+ComputeConnectedSubgraphs+ (line 21) computes all connected sub-partition in each region. Then, \verb+Attach+ (line 23) aggregate the sub-partition until we reach the number of region requested while trying to keep the best load-balance possible.

\begin{scriptsize}
\begin{algorithm}[ht!]
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\Input{A road network $G=(V,A,w)$; $n$ the number of partitions wanted; $\epsilon$ an acceptable unbalance of the partitioning.}
\Output{$\pi(G,n)$ a road network partitioning.}
\BlankLine
\Begin{
\tcp{Initialisation}
$\overrightarrow{stop} \leftarrow$ new array of size $n$\; 
\For{$i\leftarrow 1$ \KwTo $n$}{
$\gamma_i \leftarrow$ BestCandidateVertex()\;
$\overrightarrow{stop}[i] \leftarrow 1$\;
}
\tcp{Grow regions till they are blocked}
\While{$\overrightarrow{stop} \neq \vec{\emptyset}$}{
\For{$i\leftarrow 1$ \KwTo $n$}{
\If{$\overrightarrow{stop}[i] \neq 0$} {
hasGrown $\leftarrow$ Grow($\gamma_i$)\;
\If{hasGrown}{
$\overrightarrow{stop}[i] \leftarrow 0$
}
}
}
}
\tcp{Trade segments to balance partitioning}
balanced$\leftarrow$ false\;

\While{$\lnot$ balanced $\lor$ enough iterations}{
\tcp{min and max in terms of partition size}
$\gamma_i=\max \pi(G,n)$\;
$\gamma_j=\min \pi(G,n)$\;
\eIf{$|\gamma_i[V]|-\epsilon < \frac{|G[V]|}{n} < |\gamma_j[V]|+\epsilon$}{
balanced $\leftarrow$ true\;
}{
Trade($\gamma_i$, $\gamma_j$)\;
}
}
\tcp{Ensure connectivity}
\ForEach{$\gamma_i$}{
ComputeConnectedSubgraphs($\gamma_i$)\;
}
\ForEach{connected component $cc_{\gamma_i}^j$}{
Attach($cc_{\gamma_i}^j$)\;
}
}
\caption{\label{algo:spartsim}SParTSim}
\end{algorithm}
\end{scriptsize}

\section{META HEURISTICS BASED GRAPH PARTITIONING ALGORITHMS}
\label{sec:meta}

\subsection{Simulated Annealing}
\label{sec:simu-anne}

% \begin{scriptsize}
\begin{algorithm}[ht!]
  % \vspace{-.9cm}
  % \smaller{
    \caption{The Simulated Annealing Algorithm}
    \label{algo:SA}
    % \DontPrintSemicolon
    \KwIn{Initial Mapping $\lambda_0$ and Starting and Final Temperatures
$\mathcal{T}_0$, $\mathcal{T}_f$}
    \KwOut{Best Mapping $\lambda_{best}$}
    $\lambda_{current} \leftarrow \lambda_0$ \;
    $C_{current} \leftarrow objective\_function(\lambda_0)$;  //calculate initial objective function value\;
    $\lambda_{best} \leftarrow \lambda_{current}$\;
    $C_{best} \leftarrow C_{current}$\;
    \For {$i \leftarrow 0\ to\ \infty$}{
      $\mathcal{T}_{current} \leftarrow next\_temperature(\mathcal{T}_0,i)$\;
      $\lambda_{new} \leftarrow move(\lambda_{current}, \mathcal{T})$\;
      $C_{new} \leftarrow objective\_function(\lambda_{new})$\;
      $\Delta C \leftarrow C_{new} - C_{current}$\;
      $p \leftarrow acceptance\_probability(\Delta C, \mathcal{T}_{current})$\;
      \If{$\Delta C < 0$ or $r < p$}{
        \If{$C_{new} < C_{best}$}{
          $\lambda_{best} \leftarrow \lambda_{new}$;\ $C_{best} \leftarrow
          C_{new}$;\ 
          $\lambda_{current} \leftarrow \lambda_{new}$;\ $C_{current} \leftarrow C_{new}$\;
        }
      }
      \Else{
        \If{$\mathcal{T}_{current} \leq \mathcal{T}_f \| executionTime \geq maxTime$}{break}
      }
    }
    \Return $\lambda_{best}$
  % }
\end{algorithm}
% \end{scriptsize}

Simulated Annealing~\cite{kirkpatrick1983optimization} is an adaptation of the Metropolis-Hastings algorithm for solving the problem of locating a good approximation of the global optimum of a given
function, $\mathcal{F}: \mathbb{R} \rightarrow \mathbb{R}$, which has a
large search space. This large number of states, as discussed in Section~\ref{sec:form-part}, makes exhaustive enumeration to find optimal solutions, not feasible.

SA is a heuristic algorithm that explores the search space by inspecting one valid state at each iteration. Each of these inspected states are evaluated by an ``objective function'' which tells us how ``good'' or ``bad'' this state is. The ``goodness'' in an SA algorithm is problem dependent and in our case it is given by the metrics defined in Section~\ref{sec:form-obje-func}. The algorithm progresses by inspecting a candidate state at each iteration and it either accepts it as its current state or discards the state and ``moves'' on to another state. We define a move as the generation of the next candidate states and this progress is governed by a global time-varying parameter called the ``temperature'' which changes based on an ``annealing schedule''.

The algorithm always accepts a move to a better solution, i.e. a new state which has a ``better'' objective function value than the current state. When this value is worse however, the SA algorithm accepts this move with a certain ``acceptance probability'', that changes with the current temperature. When the temperature is high, the algorithm accepts moves to a worse solution with a higher probability; as the temperature reduces over time, this probability decreases as well.

 Every Simulated Annealing method is characterized by the definition of its core functions and parameters. One such core function is the ``move'' function which defines how the next candidate state is generated. We formally define a this function as,

\begin{equation}
\label{eq:move-func}
move(\lambda_G, \mathcal{T}) =  \lambda_G^{'}
\end{equation}

\noindent where $\lambda_G^{'}$ is a new partitioning scheme which also maps each intersection to exactly one partition. The move function takes as input the current partitioning scheme $\lambda_G$ and the current temperature and describes how the generation of $\lambda_G^{'}$ is done. In this paper we present a few variations of the SA algorithm, in terms of the ``move'' as follows.

\subsubsection{Standard Move}
\label{sec:opti-std}
Conventional wisdom~\cite{orsila2006parameterizing} suggests that the move function has to be random to avoid local minimums. Hence, our baseline is this conventionally accepted notion of moving about the search space through random neighbours. For each intersection in the road network graph, we choose a random partition to assign it to as follows,
\begin{equation}
\label{eq:std-move}
\lambda_G^{'}(v_i) = rand(0, p-1), \forall v_i \in V
\end{equation}
\noindent The results of this method is labelled as \textbf{SA-standard} in the graphs in the results section.

\subsubsection{Edge Labelling}
\label{sec:opti-glob-edge}
As mentioned previously in section~\ref{sec:form-obje-func}, the execution time of a distributed simulation depends on the amount of inter-partition communication. To address this, we label edges as \textbf{heavy} or \textbf{light} by virtue of their edge weights as compared to the ``rest of the edges' weights". Given a subset of edges $E_S, S \subseteq G$ we define the following metrices that we will use in our two edge labelling methods.

\begin{equation}
\label{eq:comm1-mean}
E_S^\mu = \frac{\sum\limits_{e_i \in \{E_S\}} w(e_i)}{|E_S|}
\end{equation}

\begin{equation}
\label{eq:comm1}
E_S^\sigma = \frac{\sqrt{\sum\limits_{P_i \in \{P_0,\dots,P_{p-1}\}} (w(P_i) - E_S^\mu)^2}}{p}
\end{equation}

\begin{algorithm}[ht!]
  % \vspace{-.9cm}
%  \smaller{
    \caption{Edge labelling}
    \label{algo:ga}
    % \DontPrintSemicolon
    %\KwIn{Initial Mapping $\lambda_0$ and Starting and Final Temperatures $\mathcal{T}_0$, $\mathcal{T}_f$}
    %\KwOut{Best Mapping $\lambda_{best}$}
    Calculate $E_S^\mu$ and $E_S^\sigma$ for the given $E_S$\\
    \For{$e_i \in E_S$}{
      \If{$w(e_i)-E_S^\mu \geq k*E_S^\sigma$}
      { label($e_i$) = heavy }
      \Else
      { label($e_i$) = light }
    }
 % }
\end{algorithm}

For every edge in the edge set, we check if it is ``k" sigmas away from the mean. If it is, then we label it as a heavy edge and do not allow it cross across partitions. For instance, if the edge weight of edge $e_{ij}$(between vertices $v_i$ and $v_j$) is ``k" sigmas away from the mean $E_S^\mu$, then we move vertex $j$ to the partition to which $i$ belongs to or more formally we denote it by $\lambda_G^{'}(v_j) = \lambda_G{'}(v_i)$ thereby minimizing the inter-partition communication. Since this edge labelling algorithm works for any subset $S \in G$, we define the following two flavours of the move function.

\begin{itemize}
\item \textbf{Global Edge Labelling} - In this method, we set the subset $S$ to be the entire graph $G$ itself. This gives an idea of how varied the edges are in a global setting and helps to identify the critical pieces of roads that contain a lot of traffic flowing through them. The results of this method is labelled as \textbf{SA-stats} in the graphs in the results section.
\item \textbf{Local Edge Labelling} - In contrast to the global edge labelling method, we define $S$ as the subset of vertices from the graph that are connected to some vertex $v_i$. This identifies what the most critical edges are for any given intersection and we perturb the current solution only on vertex at a time. The results of this method is labelled as \textbf{SA-all-optimizations} in the graphs in the results section.
\end{itemize}

\subsection{Genetic Algorithm}
\label{sec:gen-algo}

Genetic Algorithm is a heuristic search\{cite core paper\} algorithm that mimics the natural selection process. This heuristic algorithm is often used\{cite usage in general context\} for solving optimization problems with large search spaces and it belongs to a larger class called Evolutionary Algorithms.

\begin{algorithm}[ht!]
  % \vspace{-.9cm}
%  \smaller{
    \caption{The Genetic Algorithm}
    \label{algo:ga}
    % \DontPrintSemicolon
    %\KwIn{Initial Mapping $\lambda_0$ and Starting and Final Temperatures $\mathcal{T}_0$, $\mathcal{T}_f$}
    %\KwOut{Best Mapping $\lambda_{best}$}
    Choose an initial random population of individuals\\
    Evaluate the fitness of the individuals\\
    \While{Termination criteria not met}{
      Select the \textit{best} ``n'' individuals to be used by the genetic operators\\
      Generate new offsprings by using the genetic operators\\
      Evaluate the objective function value for these offsprings\\
      Replace the \textit{worst} ``k'' individuals of the current population with the best ``k'' individuals from the offsprings\\
    }
 % }
\end{algorithm}

In the context of the problem at hand, we follow the definition for the search space from Section~\ref{sec:form-part}, as a 2-dimensional space with one axis representing the intersections and the other the partition IDs. Hence each point in the space becomes a \textit{\{intersection,partition\}} pair. Under this definition of the search space, to characterize a genetic algorithm, we require two things :
\begin{itemize}
	\item A \textbf{genetic representation} of the solution space as shown in Figure~\ref{fig:gene-repr}
	\item An \textbf{objective function} to evaluate solutions as discussed in Equation~\ref{eq:obje-func} from Section~\ref{sec:form-obje-func}
\end{itemize}

Once this is fixed, we initialize the algorithm by generating a set of random solution vectors and assigning that as the initial population. The initial population size is problem specific and in our case it is 400 which is sufficiently a large sample and small enough to ensure fast generation of offsprings. We have chosen to generate the initial population randomly as opposed to generating it with some seed, to cover a wider range of candidates from our large search space. We then apply a set of genetic operators as discussed in Section~\ref{sec:gene-oper} to generate the next population. This process is repeated and the ``n'' best solutions are retained at every step. We terminate the algorithm once a fixed time has passed by. Algorithm~\ref{algo:ga} gives an algorithmic overview of the above mentioned process.

\subsubsection{Genetic Operators}
\label{sec:gene-oper}
Although there are a lot of genetic operators to choose from\{cite core paper and some new papers\}, we employ the \textbf{mutation} and \textbf{crossover} operators for the purpose of this comparison
\begin{itemize}
	\item \textbf{mutation} - 
	\item \textbf{crossover} - 
\end{itemize}